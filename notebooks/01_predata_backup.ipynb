{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from scipy import sparse\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import luigi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readin_train(path) :\n",
    "    \"\"\"read the csv file in\n",
    "    \n",
    "    Args :\n",
    "        path : project path\n",
    "        nrows : number of rows to read in\n",
    "    \n",
    "    Returns:\n",
    "        dataset\n",
    "        \n",
    "    \"\"\"\n",
    "    dtypes = {'Semana' : 'int32',\n",
    "              'Agencia_ID' :'int32',\n",
    "              'Canal_ID' : 'int32',\n",
    "              'Ruta_SAK' : 'int32',\n",
    "              'Cliente-ID' : 'int32',\n",
    "              'Producto_ID':'int32',\n",
    "              'Venta_hoy':'float32',\n",
    "              'Venta_uni_hoy': 'int32',\n",
    "              'Dev_uni_proxima':'int32',\n",
    "              'Dev_proxima':'float32',\n",
    "              'Demanda_uni_equil':'int32'}\n",
    "    \n",
    "    \n",
    "    names_dict = {\"Demanda_uni_equil\" : \"label\"}\n",
    "    train_dataset = pd.read_csv(path + \"/data/interim/train_sample.csv\",\n",
    "                                usecols =['Semana','Agencia_ID','Canal_ID','Ruta_SAK','Cliente_ID','Producto_ID','Demanda_uni_equil'],\n",
    "                                dtype  = dtypes)\n",
    "                                \n",
    "    train_dataset.rename(columns = names_dict, inplace = True)\n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readin_test(path) :\n",
    "    \"\"\"read the csv file in\n",
    "    \n",
    "    Args :\n",
    "        path : project path\n",
    "        nrows : number of rows to read in\n",
    "    \n",
    "    Returns:\n",
    "        dataset\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    dtypes = {'Semana' : 'int32',\n",
    "              'Agencia_ID' :'int32',\n",
    "              'Canal_ID' : 'int32',\n",
    "              'Ruta_SAK' : 'int32',\n",
    "              'Cliente-ID' : 'int32',\n",
    "              'Producto_ID':'int32',\n",
    "              'Venta_hoy':'float32',\n",
    "              'Venta_uni_hoy': 'int32',\n",
    "              'Dev_uni_proxima':'int32',\n",
    "              'Dev_proxima':'float32',\n",
    "              'Demanda_uni_equil':'int32'}\n",
    "    \n",
    "    test_dataset = pd.read_csv(path + \"/data/interim/test_sample.csv\",\n",
    "                                usecols =['Semana','Agencia_ID','Canal_ID','Ruta_SAK','Cliente_ID','Producto_ID'],\n",
    "                                dtype  = dtypes)\n",
    "    return test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pivot_table(df):\n",
    "    \"\"\"pivot the table, for each (client, product) pair, it should show week3 to week 9 demand\n",
    "    \n",
    "    Week3 : product1, client1, demand\n",
    "    Week4 : product1, client1, demand\n",
    "    Week5 : product1, client1, demand\n",
    "    Week6 : product1, client1, demand\n",
    "    Week7 : product1, client1, demand\n",
    "    Week8 : product1, client1, demand\n",
    "    \n",
    "    To\n",
    "    \n",
    "    product1, client1 : week3_demand, week4_demand, week5_demand, week6_demand, week7_demand, week8_demand\n",
    "    \n",
    "    Args :\n",
    "        Origin training dataset\n",
    "    \n",
    "    Returns :\n",
    "        Showed above\n",
    "    \"\"\"\n",
    "    df[\"label\"] = df[\"label\"].apply(np.log1p)\n",
    "    pivot_df = pd.pivot_table(data = df[[\"Semana\",\"Cliente_ID\", \"Producto_ID\", \"label\"]],\n",
    "                              values = \"label\", index = [\"Cliente_ID\", \"Producto_ID\"],\n",
    "                              columns = [\"Semana\"],  aggfunc= np.mean).reset_index()\n",
    "    \n",
    "    pivot_df.columns = [\"label_\" + str(ele) if str(ele).isdigit() else ele for ele in pivot_df.columns.values]\n",
    "    return pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_se_dist(df):\n",
    "    \"\"\"generate statistical distribution value for Producto_ID(product_id) and Cliente_ID(client_id) pair\n",
    "    \n",
    "    The main issue of this dataset is most features are in nominal format, ex. ID\n",
    "    So it's better to generate some numerical distribution value for reasonble nominal feature pair    \n",
    "    \n",
    "    Args : train_dataset\n",
    "    \n",
    "    Returns : dist info provided by train_dataset\n",
    "    \"\"\"\n",
    "    df_se_pro_cli_dist = df.groupby([\"Cliente_ID\",\"Producto_ID\"],as_index = False).\\\n",
    "                        agg(['count','sum', 'min', 'max','median','mean']).reset_index()\n",
    "    df_se_pro_cli_dist.columns = [\"_\".join(ele) + \"_spc\" if ele[1] != \"\" \n",
    "                               else ele[0] for ele in df_se_pro_cli_dist.columns.values]\n",
    "        \n",
    "    df_se_pro_dist = df.drop(\"Cliente_ID\",1).groupby([\"Producto_ID\"],as_index = False).\\\n",
    "                        agg(['count','sum', 'min', 'max','median','mean']).reset_index()\n",
    "    df_se_pro_dist.columns = [\"_\".join(ele) + \"_sp\" if ele[1] != \"\" \n",
    "                               else ele[0] for ele in df_se_pro_dist.columns.values]\n",
    "        \n",
    "    df_se_cli_dist = df.drop(\"Producto_ID\",1).groupby([\"Cliente_ID\"],as_index = False).\\\n",
    "                        agg(['count','sum', 'min', 'max','median','mean']).reset_index()\n",
    "    df_se_cli_dist.columns = [\"_\".join(ele) + \"_sc\" if ele[1] != \"\" \n",
    "                           else ele[0] for ele in df_se_cli_dist.columns.values]\n",
    "        \n",
    "    return df_se_pro_cli_dist, df_se_pro_dist, df_se_cli_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_pro_cli_dist(df):\n",
    "    df_cli_pro_dist = df[[\"Cliente_ID\",\"Producto_ID\",'label']].\\\n",
    "                 groupby([\"Cliente_ID\",\"Producto_ID\"],as_index = False).\\\n",
    "                 agg(['count','sum', 'min', 'max','median','mean']).reset_index()\n",
    "    df_cli_pro_dist.columns = [\"_\".join(ele) + \"_cp\" if ele[1] != \"\" \n",
    "                       else ele[0] for ele in df_cli_pro_dist.columns.values]\n",
    "    return df_cli_pro_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_df_dist(df, dist):\n",
    "    \"\"\"merge the dist info with train_dataset or test_dataset\"\"\"\n",
    "    dist_join_key = [ele for ele in dist.columns if ele.split(\"_\")[0] != \"label\"]\n",
    "    df = df.merge(dist, how = 'left', on = dist_join_key)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_lag(df):\n",
    "    \"\"\"create time lag between demand among each week\n",
    "    \n",
    "    The week10 is held out data and week11 is the data we need to predict\n",
    "    So for week10 we create lag1 and lag2 between week3 ~ week8  \n",
    "    So for week11 we create lag1 and lag2 between week4 ~ week9\n",
    "    \n",
    "    It should be after pivot_table when training and test data have already been merged with dist infomation\n",
    "    \n",
    "    Args : \n",
    "        training data or test data already merged with dist info\n",
    "    \"\"\"\n",
    "    periods = [(3,8), (4,9)]\n",
    "    lag_time = [1, 2]\n",
    "    \n",
    "    for period in periods:\n",
    "        for lag in lag_time:\n",
    "            for index in range(period[0], period[1] + 1 - lag):\n",
    "                df['label_' + str(index + lag) + '_min_' + str(index )] = df[\"label_\" + str(index + lag)].subtract(df[\"label_\" + str(index)])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def external_info(path):\n",
    "    \"\"\"add additional information from /external, this data were provided by Kaggle\n",
    "    \n",
    "    External Source name :\n",
    "        cliente_tabla\n",
    "        producto_tabla\n",
    "        town_tabla\n",
    "    \n",
    "    \"\"\"\n",
    "    producto_tabla = pd.read_csv(path + \"/data/external/producto_tabla.csv\")\n",
    "    town_state = pd.read_csv(path + \"/data/external/town_state.csv\")\n",
    "    \n",
    "    town_state['town_id'] = town_state['Town'].str.split()\n",
    "    town_state['town_id'] = town_state['Town'].str.split(expand = True)\n",
    "    town_state = pd.concat([town_state, pd.get_dummies(town_state[[\"town_id\",\"State\"]], prefix=['town_id', 'state_id'])],axis = 1)\n",
    "    town_state.drop([\"Town\", \"State\", \"town_id\"], axis = 1, inplace = True)\n",
    "    \n",
    "    reg_weight = r\" (\\d{1,10})g \"\n",
    "    reg_piece = r\" (\\d{1,10})p \"\n",
    "    producto_tabla[\"weight\"] = producto_tabla[\"NombreProducto\"].apply(lambda x: re.findall(reg_weight, x)[0] if re.search(reg_weight, x) else np.nan)\n",
    "    producto_tabla[\"piece\"] = producto_tabla[\"NombreProducto\"].apply(lambda x: re.findall(reg_piece, x)[0] if re.search(reg_piece, x) else np.nan)\n",
    "    producto_tabla[\"wei_per_piece\"] = producto_tabla[\"weight\"].astype(float).divide(producto_tabla[\"piece\"].astype(float))\n",
    "    producto_tabla.drop(\"NombreProducto\", axis = 1, inplace = True)\n",
    "    \n",
    "    return town_state, producto_tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_dist_train_test(train_df, test_df, pivot_table, gen_se_dist, gen_pro_cli_dist, external_info):\n",
    "    \"\"\"generate dist information on training data, merge the distribution with both training data and test data\n",
    "    \n",
    "    The Data flow should look like this:\n",
    "        train_df ==> pivot_table ==> gen_se_dist ====> get_dist_info ==> merge info(train and test)\n",
    "                |               |                   |\n",
    "                |               |                   |\n",
    "                |                ==> create_lag ====|\n",
    "                |                                   |\n",
    "                |                                   |\n",
    "                 ==> gen_pro_cli_dist ==============|\n",
    "                |                                   |\n",
    "                |                                   |\n",
    "                 ==> external_info =================|\n",
    "\n",
    "    So after these process, the train_df.shape[0] will decrease, while test_df.shape[0] will not change\n",
    "    \"\"\"\n",
    "    \n",
    "    df_cli_pro_dist = gen_pro_cli_dist(train_df)\n",
    "    \n",
    "    train_pivot = pivot_table(train_df)\n",
    "    df_se_pro_cli_dist, df_se_pro_dist, df_se_cli_dist = gen_se_dist(train_pivot)\n",
    "    df_lag = create_lag(train_pivot)\n",
    "\n",
    "    dist_list = [df_cli_pro_dist, df_lag, df_se_pro_cli_dist, df_se_pro_dist, df_se_cli_dist]\n",
    "    \n",
    "    for dist in dist_list:\n",
    "        dist_join_key = [ele for ele in dist.columns if ele.split(\"_\")[0] != \"label\"]\n",
    "        train_df = train_df.merge(dist, on = dist_join_key, how = 'left')\n",
    "        test_df = test_df.merge(dist, on = dist_join_key, how = 'left')\n",
    "    \n",
    "    \n",
    "    town_state, producto_tabla = external_info(path)\n",
    "    \n",
    "    train_df = train_df.merge(town_state, on = \"Agencia_ID\", how = 'left')\n",
    "    train_df = train_df.merge(producto_tabla, on = \"Producto_ID\", how = 'left')\n",
    "    \n",
    "    test_df = test_df.merge(town_state, on = \"Agencia_ID\", how = 'left')\n",
    "    test_df = test_df.merge(producto_tabla, on = \"Producto_ID\", how = 'left')\n",
    "    \n",
    "    train_df.drop([\"Semana\", \"Agencia_ID\", \"Canal_ID\", \"Ruta_SAK\", \"Cliente_ID\", \"Producto_ID\", \"label\"], axis = 1, inplace = True)\n",
    "    train_df.drop_duplicates(inplace = True)\n",
    "    \n",
    "    test_df.drop([\"Semana\", \"Agencia_ID\", \"Canal_ID\", \"Ruta_SAK\", \"Cliente_ID\", \"Producto_ID\"], axis = 1, inplace = True)\n",
    "        \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_name(column_name, type_name):\n",
    "    \"\"\"filter name for train and test data for model\n",
    "    \n",
    "    Args :\n",
    "        column_name : colname for training_df and test_df\n",
    "        type_name : can either be \"train\", \"test\", \"hold\"\n",
    "    \n",
    "    Returns:\n",
    "        filtered column based on type\n",
    "    \"\"\"\n",
    "    filtered_name = []\n",
    "    if type_name == \"train\":\n",
    "        # 8, 9 can be shown in featurem label is 9\n",
    "        for name in column_name:\n",
    "            if str(8) in name.split(\"_\") or str(9) in name.split(\"_\"):\n",
    "                pass\n",
    "            else:\n",
    "                filtered_name.append(name)\n",
    "    elif type_name == \"test\":\n",
    "        # 3, 9 can be shown in featurem label is 10(label on Kaggle, so no local label)\n",
    "        for name in column_name:\n",
    "            if str(3) in name.split(\"_\") or str(9) in name.split(\"_\"):\n",
    "                pass\n",
    "            else:\n",
    "                filtered_name.append(name)\n",
    "    else:\n",
    "        # 3, 4 can be shown in featurem label is 11(label on Kaggle, so no local label)\n",
    "        for name in column_name:\n",
    "            if str(3) in name.split(\"_\") or str(4) in name.split(\"_\"):\n",
    "                pass\n",
    "            else:\n",
    "                filtered_name.append(name)\n",
    "    \n",
    "    return filtered_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_train_hold_test(train_df, test_df):\n",
    "    \"\"\"based on the specifity on this problem, we should split train test data based on time factor\n",
    "    \n",
    "    training_data : week3, week4, week5, week6, week7 ==> week9\n",
    "    hold_out_data : week4, week5, week6, week7, week8 ==> week10 (get result from Kaggle public board)\n",
    "    test_data : week5, week6, week7, week8, week9 ==> week11 (get result from Kaggle private board)\n",
    "    \n",
    "    generate (training_data, train_label), hold_out_data and test_data\n",
    "    \"\"\"\n",
    "    train_data = train_df[filter_name(train_df.columns.values, \"train\")]\n",
    "    train_label = train_df[\"label_9\"]\n",
    "    \n",
    "    hold_data = train_df[filter_name(train_df.columns.values, \"hold\")]\n",
    "    test_data = test_df[filter_name(test_df.columns.values, \"test\")]\n",
    "    \n",
    "    return train_data, train_label, hold_data, test_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_whole():\n",
    "    \"\"\"prepare for the train_df and test_df\"\"\"\n",
    "    path = \"~/bimbo_kaggle_competition\"\n",
    "    train_dataset = readin_train(path)\n",
    "    test_dataset = readin_test(path)\n",
    "    train_dataset, test_dataset = gen_dist_train_test(train_dataset, test_dataset, \n",
    "                                                      pivot_table, \n",
    "                                                      gen_se_dist, \n",
    "                                                      gen_pro_cli_dist,\n",
    "                                                      external_info)\n",
    "    train_data, train_label, hold_data, test_data = prepare_train_hold_test(train_dataset, test_dataset)\n",
    "    \n",
    "    train_data.to_csv(path + \"/data/processed/train.csv\", index = False)\n",
    "    train_label.to_csv(path + \"/data/processed/train_label.csv\", index = False)\n",
    "    hold_data.to_csv(path + \"/data/processed/hold.csv\", index = False)\n",
    "    test_data.to_csv(path + \"/data/processed/test_label.csv\", index = False)\n",
    "    \n",
    "    print \"Data preparation is done\"\n",
    "    print \"%s of features is created\" %train_data.shape[0]\n",
    "    \n",
    "    print \"training data size : %s\" %train_data.shape[1]\n",
    "    print \"holdout data size : %s\" %hold_data.shape[1]\n",
    "    print \"test data size : %s\" %test_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, train_label, hold_data, test_data = prepare_whole()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_count_cp</th>\n",
       "      <th>label_sum_cp</th>\n",
       "      <th>label_min_cp</th>\n",
       "      <th>label_max_cp</th>\n",
       "      <th>label_median_cp</th>\n",
       "      <th>label_mean_cp</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "      <th>label_5</th>\n",
       "      <th>label_6</th>\n",
       "      <th>...</th>\n",
       "      <th>state_id_SONORA</th>\n",
       "      <th>state_id_TABASCO</th>\n",
       "      <th>state_id_TAMAULIPAS</th>\n",
       "      <th>state_id_TLAXCALA</th>\n",
       "      <th>state_id_VERACRUZ</th>\n",
       "      <th>state_id_YUCATÁN</th>\n",
       "      <th>state_id_ZACATECAS</th>\n",
       "      <th>weight</th>\n",
       "      <th>piece</th>\n",
       "      <th>wei_per_piece</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>360</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 401 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label_count_cp  label_sum_cp  label_min_cp  label_max_cp  label_median_cp  \\\n",
       "0               1             1             1             1              1.0   \n",
       "1               1             9             9             9              9.0   \n",
       "2               1             2             2             2              2.0   \n",
       "3               1             1             1             1              1.0   \n",
       "4               1             2             2             2              2.0   \n",
       "\n",
       "   label_mean_cp   label_3  label_4  label_5   label_6      ...        \\\n",
       "0            1.0       NaN      NaN      NaN       NaN      ...         \n",
       "1            9.0       NaN      NaN      NaN       NaN      ...         \n",
       "2            2.0  1.098612      NaN      NaN       NaN      ...         \n",
       "3            1.0       NaN      NaN      NaN  0.693147      ...         \n",
       "4            2.0       NaN      NaN      NaN       NaN      ...         \n",
       "\n",
       "   state_id_SONORA  state_id_TABASCO  state_id_TAMAULIPAS  state_id_TLAXCALA  \\\n",
       "0                0                 0                    0                  0   \n",
       "1                0                 0                    0                  0   \n",
       "2                0                 0                    0                  0   \n",
       "3                0                 0                    0                  0   \n",
       "4                0                 0                    0                  0   \n",
       "\n",
       "   state_id_VERACRUZ  state_id_YUCATÁN  state_id_ZACATECAS  weight  piece  \\\n",
       "0                  0                 0                   0     540    NaN   \n",
       "1                  0                 0                   0      62      1   \n",
       "2                  0                 0                   0     123    NaN   \n",
       "3                  0                 0                   0     255    NaN   \n",
       "4                  0                 0                   0     360    NaN   \n",
       "\n",
       "   wei_per_piece  \n",
       "0            NaN  \n",
       "1           62.0  \n",
       "2            NaN  \n",
       "3            NaN  \n",
       "4            NaN  \n",
       "\n",
       "[5 rows x 401 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 401)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
