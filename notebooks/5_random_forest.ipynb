{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random_forest_regressor\n",
    "# extra_tree_regressor\n",
    "# sklearn.svm.SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble.forest import RandomForestRegressor\n",
    "from sklearn import grid_search\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_predata.ipynb                   preprocessed_products.csv\r\n",
      "3_xgb.ipynb                       ruta_for_cliente_producto.csv\r\n",
      "3_xgb_prediction.ipynb            \u001b[0m\u001b[01;34mstack_sub\u001b[0m/\r\n",
      "44fea_bst.model                   submission_10_new.csv\r\n",
      "4_keras_nn.ipynb                  submission_11_new.csv\r\n",
      "5_random_forest.ipynb             submission_44fea.csv\r\n",
      "6_stack_model.ipynb               submission_nn.csv\r\n",
      "agencia_for_cliente_producto.csv  submission_nn_xgb\r\n",
      "canal_for_cliente_producto.csv    train_pivot_56789_to_10_44fea.pickle\r\n",
      "model_nn_10_after_l2reg.h5        train_pivot_56789_to_10_new.pickle\r\n",
      "model_nn_10.h5                    train_pivot_6789_to_11_new.pickle\r\n",
      "model_nn_10_whole.h5              train_pivot_xgb_time1_44fea.csv\r\n",
      "\u001b[01;34mold_submission\u001b[0m/                   train_pivot_xgb_time1.csv\r\n",
      "\u001b[01;34morigin\u001b[0m/                           train_pivot_xgb_time2_38fea.csv\r\n",
      "pivot_test.pickle                 train_pivot_xgb_time2.csv\r\n",
      "pivot_train_with_nan.pickle\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtypes = {'agen_for_log_de':'float32',\n",
    "        'ruta_for_log_de':'float32',\n",
    "        'cliente_for_log_de':'float32',\n",
    "        'producto_for_log_de':'float32',\n",
    "        'agen_ruta_for_log_de':'float32',\n",
    "        'agen_cliente_for_log_de':'float32',\n",
    "        'agen_producto_for_log_de':'float32',\n",
    "        'ruta_cliente_for_log_de':'float32',\n",
    "        'ruta_producto_for_log_de':\"float32\",\n",
    "        'cliente_producto_for_log_de':'float32',\n",
    "        'cliente_for_log_sum':'float32',\n",
    "        'corr':'float32',\n",
    "        't_min_1':'float32',\n",
    "        't_min_2':'float32',\n",
    "        't_min_3':'float32',\n",
    "        't_min_4':'float32',\n",
    "        't_min_5':'float32',\n",
    "        't1_min_t2':'float32',\n",
    "        't1_min_t3':'float32',\n",
    "        't1_min_t4':'float32',\n",
    "        't1_min_t5':'float32',\n",
    "        't2_min_t3':'float32',\n",
    "        't2_min_t4':'float32',\n",
    "        't2_min_t5':'float32',\n",
    "        't3_min_t4':'float32',\n",
    "        't3_min_t5':'float32',\n",
    "        't4_min_t5':'float32',\n",
    "        'LR_prod':'float32',\n",
    "        'LR_prod_corr':'float32',\n",
    "        'target':'float32',\n",
    "        't_m_5_cum':'float32',\n",
    "        't_m_4_cum' :'float32',\n",
    "        't_m_3_cum':'float32',\n",
    "        't_m_2_cum':'float32',\n",
    "        't_m_1_cum':'float32',\n",
    "        'NombreCliente':'int32',\n",
    "        'weight':'float32',\n",
    "        'weight_per_piece':'float32',\n",
    "        'pieces':'float32'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors_10 = ['agen_for_log_de', 'ruta_for_log_de', 'cliente_for_log_de',\n",
    "       'producto_for_log_de', 'agen_ruta_for_log_de',\n",
    "       'agen_cliente_for_log_de', 'agen_producto_for_log_de',\n",
    "       'ruta_cliente_for_log_de', 'ruta_producto_for_log_de',\n",
    "       'cliente_producto_for_log_de', 'cliente_for_log_sum', 'corr',\n",
    "       't_min_1', 't_min_2', 't_min_3', 't_min_4', 't_min_5', 't1_min_t2',\n",
    "       't1_min_t3', 't1_min_t4', 't1_min_t5', 't2_min_t3', 't2_min_t4',\n",
    "       't2_min_t5', 't3_min_t4', 't3_min_t5', 't4_min_t5', 'LR_prod',\n",
    "       'LR_prod_corr', 't_m_5_cum', 't_m_4_cum', 't_m_3_cum',\n",
    "       't_m_2_cum', 't_m_1_cum', 'NombreCliente', 'weight',\n",
    "       'weight_per_piece', 'pieces']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors_10_target = ['agen_for_log_de', 'ruta_for_log_de', 'cliente_for_log_de',\n",
    "       'producto_for_log_de', 'agen_ruta_for_log_de',\n",
    "       'agen_cliente_for_log_de', 'agen_producto_for_log_de',\n",
    "       'ruta_cliente_for_log_de', 'ruta_producto_for_log_de',\n",
    "       'cliente_producto_for_log_de', 'cliente_for_log_sum', 'corr',\n",
    "       't_min_1', 't_min_2', 't_min_3', 't_min_4', 't_min_5', 't1_min_t2',\n",
    "       't1_min_t3', 't1_min_t4', 't1_min_t5', 't2_min_t3', 't2_min_t4',\n",
    "       't2_min_t5', 't3_min_t4', 't3_min_t5', 't4_min_t5', 'LR_prod',\n",
    "       'LR_prod_corr', 't_m_5_cum', 't_m_4_cum', 't_m_3_cum',\n",
    "       't_m_2_cum', 't_m_1_cum', 'NombreCliente', 'weight',\n",
    "       'weight_per_piece', 'pieces','target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = lambda x : (x-x.mean())/x.std(ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_pivot_xgb_time1 = pd.read_csv('train_pivot_xgb_time1.csv',dtype=dtypes,usecols = predictors_10_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['agen_for_log_de', 'ruta_for_log_de', 'cliente_for_log_de',\n",
       "       'producto_for_log_de', 'agen_ruta_for_log_de',\n",
       "       'agen_cliente_for_log_de', 'agen_producto_for_log_de',\n",
       "       'ruta_cliente_for_log_de', 'ruta_producto_for_log_de',\n",
       "       'cliente_producto_for_log_de', 'cliente_for_log_sum', 'corr',\n",
       "       't_min_1', 't_min_2', 't_min_3', 't_min_4', 't_min_5', 't1_min_t2',\n",
       "       't1_min_t3', 't1_min_t4', 't1_min_t5', 't2_min_t3', 't2_min_t4',\n",
       "       't2_min_t5', 't3_min_t4', 't3_min_t5', 't4_min_t5', 'LR_prod',\n",
       "       'LR_prod_corr', 'target', 't_m_5_cum', 't_m_4_cum', 't_m_3_cum',\n",
       "       't_m_2_cum', 't_m_1_cum', 'NombreCliente', 'weight',\n",
       "       'weight_per_piece', 'pieces'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pivot_xgb_time1.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pivot_56789_to_10 = pd.read_pickle('train_pivot_56789_to_10_new.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Cliente_ID', 'Producto_ID', 'id', 'Semana', 'Agencia_ID',\n",
       "       'Canal_ID', 'Ruta_SAK', 'agen_for_log_de', 'ruta_for_log_de',\n",
       "       'cliente_for_log_de', 'producto_for_log_de', 'agen_ruta_for_log_de',\n",
       "       'agen_cliente_for_log_de', 'agen_producto_for_log_de',\n",
       "       'ruta_cliente_for_log_de', 'ruta_producto_for_log_de',\n",
       "       'cliente_producto_for_log_de', 'cliente_for_log_sum', 'corr',\n",
       "       't_min_1', 't_min_2', 't_min_3', 't_min_4', 't_min_5', 't1_min_t2',\n",
       "       't1_min_t3', 't1_min_t4', 't1_min_t5', 't2_min_t3', 't2_min_t4',\n",
       "       't2_min_t5', 't3_min_t4', 't3_min_t5', 't4_min_t5', 'LR_prod',\n",
       "       'LR_prod_corr', 't_m_5_cum', 't_m_4_cum', 't_m_3_cum', 't_m_2_cum',\n",
       "       't_m_1_cum', 'NombreCliente', 'weight', 'weight_per_piece', 'pieces'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pivot_56789_to_10.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_dataset_10(train_dataset,test_dataset):\n",
    "    train_dataset_normalize = train_dataset[predictors_10]\n",
    "    train_dataset_normalize['label'] = 0    \n",
    "    \n",
    "    test_dataset_normalize = test_dataset[predictors_10]\n",
    "    test_dataset_normalize['label'] = 1\n",
    "    \n",
    "    whole_dataset = pd.concat([train_dataset_normalize,test_dataset_normalize],copy = False)\n",
    "    whole_dataset_normalize = whole_dataset.apply(f,axis = 0)\n",
    "    \n",
    "    train_dataset_normalize = whole_dataset_normalize.loc[whole_dataset.label == 0]\n",
    "    test_dataset_normalize = whole_dataset_normalize.loc[whole_dataset.label==1]\n",
    "    \n",
    "    train_dataset_normalize.drop(['label'],axis = 1,inplace = True)\n",
    "    test_dataset_normalize.drop(['label'],axis =1,inplace = True)\n",
    "    \n",
    "    train_dataset_normalize['target'] = train_dataset['target'] \n",
    "    \n",
    "#     target = train_dataset['target']\n",
    "    return train_dataset_normalize,test_dataset_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "train_dataset_10_normalize, test_dataset_10_normalize = normalize_dataset_10(train_pivot_xgb_time1,\n",
    "                                                                          train_pivot_56789_to_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset_10_normalize.to_csv('train_dataset_10_normalize.csv')\n",
    "test_dataset_10_normalize.to_csv('test_dataset_10_normalize.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_10_normalize = pd.read_csv('train_dataset_10_normalize.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20768652, 39)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_10_normalize.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare for stack model training data, 10% sample, 40 bagging\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.externals import joblib\n",
    "\n",
    "# for i in range(20):\n",
    "#     train_dataset_10_normalize.fillna(-99,inplace = True)\n",
    "#     train_dataset_10_normalize.reset_index(drop = True, inplace = True)\n",
    "\n",
    "#     train_dataset_10_normalize_sample = train_dataset_10_normalize[predictors_10_target].sample(2000000)\n",
    "\n",
    "#     train_label_10 = train_dataset_10_normalize_sample['target']\n",
    "#     train_feature_10 = train_dataset_10_normalize_sample.drop(['target'],axis = 1)\n",
    "\n",
    "#     gc.collect()\n",
    "\n",
    "#     clf = RandomForestRegressor(n_estimators=1400,\n",
    "#                                  n_jobs = 11,\n",
    "#                                  max_depth = 22,\n",
    "#                                  max_features = 'log2',\n",
    "#                                  bootstrap = True)\n",
    "\n",
    "#     clf.fit(train_feature_10,train_label_10)\n",
    "#     print 'model already fitted'\n",
    "    \n",
    "#     # save the model to disk\n",
    "#     filename = 'RF'+str(i)+'.model'\n",
    "#     joblib.dump(clf, filename)\n",
    "    \n",
    "# print 'finished'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# submission_10 = pd.DataFrame()\n",
    "# i = 0\n",
    "# clf = joblib.load('filename.pkl') \n",
    "# submission_10['predict_' + str(i)] = clf.predict(train_dataset_10_normalize[predictors_10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset_10_normalize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b901674e841c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_dataset_10_normalize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_dataset_10_normalize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset_10_normalize' is not defined"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "train_dataset_10_normalize.fillna(-99,inplace = True)\n",
    "train_dataset_10_normalize.reset_index(drop = True, inplace = True)\n",
    "\n",
    "gc.collect()\n",
    "submission_10 = pd.DataFrame()\n",
    "\n",
    "for i in range(20):\n",
    "    train_dataset_10_normalize_sample = train_dataset_10_normalize[predictors_10_target].sample(20000)\n",
    "\n",
    "    train_label_10 = train_dataset_10_normalize_sample['target']\n",
    "    train_feature_10 = train_dataset_10_normalize_sample.drop(['target'],axis = 1)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    clf = RandomForestRegressor(n_estimators=40,\n",
    "                                n_jobs = 1,\n",
    "                                max_depth = 6,\n",
    "                                max_features = 'log2',\n",
    "                                bootstrap = False,\n",
    "                                verbose = 1)\n",
    "\n",
    "    clf.fit(train_feature_10,train_label_10)\n",
    "\n",
    "    submission_10['predict_' + str(i)] = clf.predict(train_dataset_10_normalize[predictors_10])\n",
    "    print submission_10.head()\n",
    "    submission_10['predict_' + str(i)].loc[train_dataset_10_normalize_sample.index.values] = np.nan\n",
    "    print clf.score(train_dataset_10_normalize[predictors_10],train_dataset_10_normalize['target'])\n",
    "    print str(i) + '__predicting finished!'\n",
    "    gc.collect()\n",
    "\n",
    "print 'finished'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # gird search \n",
    "# # create model\n",
    "# # svr_rbf = SVR(kernel='rbf', C=1e3, epsilon = 0.1,gamma=0.1)\n",
    "# random_forest_regressor = RandomForestRegressor(n_jobs = 5,\n",
    "#                                                 verbose = True,\n",
    "# #                                                 max_depth = 5,\n",
    "#                                                bootstrap = True)\n",
    "\n",
    "# # grid search epochs, batch size and optimizer# use a full grid over all parameters\n",
    "# param_grid = {\"n_estimators\":[1400,1600],\n",
    "#               \"max_depth\": [12,20,25],\n",
    "# #               \"max_depth\": [5,None],\n",
    "#               \"max_features\": ['log2','sqrt']}\n",
    "# #               \"min_samples_leaf\": [5, 10]}\n",
    "# #               \"min_samples_split\": [10,15,100],              \n",
    "\n",
    "# # more complex para:\n",
    "# # gamma = numpy.array([50, 100, 150])\n",
    "# # degree = numpy.array([5, 10, 20])\n",
    "# # param_grid = dict(kernel=kernel, C = C, batch_size=batches, init=init)\n",
    "\n",
    "\n",
    "# grid = grid_search.GridSearchCV(random_forest_regressor, param_grid=param_grid)\n",
    "# grid_result = grid.fit(train_nn_time1, label_nn_time1)\n",
    "\n",
    "# # summarize results\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# for params, mean_score, scores in grid_result.grid_scores_:\n",
    "#     print(\"%f (%f) with: %r\" % (scores.mean(), scores.std(), params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] no parameters to be set .........................................\n"
     ]
    }
   ],
   "source": [
    "# # evaluate model with standardized dataset\n",
    "# # numpy.random.seed(seed)\n",
    "seed = 42\n",
    "\n",
    "# # parameter for svr:\n",
    "# # C = penalty before 'square loss', the larger the C, the more bias and less variance\n",
    "\n",
    "\n",
    "train_dataset_10_normalize.fillna(-99,inplace = True)\n",
    "train_dataset_10_normalize.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# train_dataset_10_normalize_sample = train_dataset_10_normalize[predictors_10_target]\n",
    "\n",
    "train_label_10 = train_dataset_10_normalize['target']\n",
    "train_feature_10 = train_dataset_10_normalize.drop(['target'],axis = 1)\n",
    "# # gamma means parameter before(in) gussian kernel, the larger the gamma, the larger the bias and less variance\n",
    "clf = RandomForestRegressor(n_estimators=1400,\n",
    "                             n_jobs = 11,\n",
    "                             max_depth = 22,\n",
    "                             max_features = 'log2',\n",
    "                             bootstrap = True)\n",
    "\n",
    "\n",
    "\n",
    "kfold = KFold(n=len(train_label_10), n_folds=5, random_state=seed)\n",
    "results = cross_val_score(clf,train_feature_10, train_label_10,scoring='mean_squared_error' ,cv=kfold,verbose = 3)\n",
    "print results\n",
    "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
